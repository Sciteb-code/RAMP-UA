{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis with the OpenCL RAMP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import opencl modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import yaml # pyyaml library for reading the parameters.yml file\n",
    "import os\n",
    "import pandas as pd\n",
    "import unittest\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from microsim.opencl.ramp.run import run_headless\n",
    "from microsim.opencl.ramp.snapshot_convertor import SnapshotConvertor\n",
    "from microsim.opencl.ramp.snapshot import Snapshot\n",
    "from microsim.opencl.ramp.params import Params, IndividualHazardMultipliers, LocationHazardMultipliers\n",
    "from microsim.opencl.ramp.simulator import Simulator\n",
    "from microsim.opencl.ramp.disease_statuses import DiseaseStatus\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#import experiments_functions  # For the ones outside the class\n",
    "from experiments_functions import Functions # Some additional notebook-specific functions required (functions.py)\n",
    "\n",
    "# Useful for connecting to this kernel\n",
    "#%connect_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup params for all runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the parameters file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS_FILENAME = \"default.yml\"\n",
    "#with open(os.path.join(\"../../\",\"model_parameters\", PARAMETERS_FILENAME)) as f:\n",
    "#    parameters = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    \n",
    "#sim_params = parameters[\"microsim\"]  # Parameters for the dynamic microsim (python)\n",
    "#calibration_params = parameters[\"microsim_calibration\"]\n",
    "#disease_params = parameters[\"disease\"]  # Parameters for the disease model (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the parameters for the OpenCL model. (See [main.py](https://github.com/Urban-Analytics/RAMP-UA/blob/052861cc51be5bc1827c85bb827209f0df73c685/microsim/main.py#L262) for an example of how this is done in the code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS_FILENAME = \"default.yml\"  # Creates default parameters using the default.yml file\n",
    "PARAMS = Functions.create_parameters(\n",
    "    parameters_file=os.path.join(\"../../\",\"model_parameters\", PARAMETERS_FILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get snapshot path\n",
    "**NB** this is the path to the OpenCL snapshot file generated by running `microsim/main.py`. To run with new population data just re-run `main.py --opencl` without the `--use-cache` option, so that it regenerates a new snapshot file and writes it to this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENCL_DIR = \"../../microsim/opencl\"\n",
    "SNAPSHOT_FILEPATH = os.path.join(OPENCL_DIR, \"snapshots\", \"cache.npz\")\n",
    "assert os.path.isfile(SNAPSHOT_FILEPATH), f\"Snapshot doesn't exist: {SNAPSHOT_FILEPATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Default OpenCL simulation for multiple repetitions\n",
    "\n",
    "This shows what happens with the 'default' (manually calibrated) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "repetitions = 10\n",
    "num_seed_days = 10\n",
    "use_gpu=False\n",
    "\n",
    "results = Functions.run_opencl_model_multi(\n",
    "    repetitions=10, \n",
    "    iterations=100, \n",
    "    params=Functions.create_parameters(parameters_file=os.path.join(\"../..\", \"model_parameters\", \"default.yml\")),\n",
    "    num_seed_days=10,\n",
    "    store_detailed_counts=True,\n",
    "    opencl_dir=os.path.join(\"../..\", \"microsim\", \"opencl\"),\n",
    "    snapshot_filepath=os.path.join(\"../..\", \"microsim\", \"opencl\", \"snapshots\", \"cache.npz\"),\n",
    "    multiprocess=False\n",
    ")\n",
    "\n",
    "summaries = [x[0] for x in results]\n",
    "final_results = [x[1] for x in results]\n",
    "print(\"Finished\")\n",
    "\n",
    "# To make it convenient to reload later:\n",
    "pickle.dump( summaries, open( \"./summaries.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot output summary data\n",
    "\n",
    "### Total counts of disease status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summaries(summaries, plot_type=\"error_bars\"):\n",
    "\n",
    "    #fig, ax = plt.subplots(1, len(DiseaseStatus), sharey=True)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,7))\n",
    "    \n",
    "    # Work out the number of repetitions and iterations\n",
    "    iters, reps = _get_iters_and_reps(summaries)\n",
    "    x = range(iters)\n",
    "\n",
    "    for d, disease_status in enumerate(DiseaseStatus):\n",
    "        if disease_status==DiseaseStatus.Susceptible or disease_status==DiseaseStatus.Recovered:\n",
    "            continue\n",
    "        # Calculate the mean and standard deviation\n",
    "        matrix = np.zeros(shape=(reps,iters))\n",
    "        for rep in range(reps):\n",
    "            matrix[rep] = summaries[rep].total_counts[d]\n",
    "        mean = np.mean(matrix, axis=0)\n",
    "        sd = np.std(matrix, axis=0)\n",
    "        if plot_type == \"error_bars\":\n",
    "            ax.errorbar(x, mean, sd, label=f\"{disease_status}\" )\n",
    "        elif plot_type == \"lines\":\n",
    "            for rep in range(reps):\n",
    "                ax.plot(x, matrix[rep], label=f\"{disease_status} {rep}\", \n",
    "                        color=plt.cm.get_cmap(\"hsv\", len(DiseaseStatus))(d) )\n",
    "                \n",
    "    ax.legend() \n",
    "    ax.set_title(\"Disease Status\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Number of cases\")\n",
    "\n",
    "def _get_iters_and_reps(summaries):\n",
    "    reps = len(summaries)\n",
    "    iters = len(summaries[0].total_counts[0])\n",
    "    return (iters, reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summaries(summaries=summaries, plot_type=\"error_bars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_summaries(summaries=summaries, plot_type=\"lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disease statuses by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def plot_disease_status_by_age(summaries):\n",
    "\n",
    "    #fig, ax = plt.subplots(1, len(DiseaseStatus), sharey=True)\n",
    "    fig, ax = plt.subplots(int(len(DiseaseStatus)/2), int(len(DiseaseStatus)/2), \n",
    "                           figsize=(15,11), tight_layout=True)\n",
    "    iters, reps = _get_iters_and_reps(summaries)\n",
    "    x = range(iters)\n",
    "    age_thresholds = summaries[0].age_thresholds\n",
    "\n",
    "    for d, disease_status in enumerate(DiseaseStatus):\n",
    "        lower_age_bound = 0\n",
    "        for age_idx in range(len(age_thresholds)):\n",
    "            matrix = np.zeros(shape=(reps, iters))\n",
    "            for rep in range(reps):\n",
    "                #matrix[age_idx][rep][it] = summaries[rep].age_counts[str(disease_status)][age_idx][it]\n",
    "                matrix[rep] = summaries[rep].age_counts[str(disease_status)][age_idx]\n",
    "            mean = np.mean(matrix, axis=0)\n",
    "            sd = np.std(matrix, axis=0)\n",
    "            ax.flat[d].errorbar(x, mean, sd, label=f\"{lower_age_bound} - {age_thresholds[age_idx]}\" )\n",
    "            lower_age_bound = age_thresholds[age_idx]\n",
    "                \n",
    "            ax.flat[d].legend() \n",
    "            ax.flat[d].set_title(f\"{str(disease_status)}\")\n",
    "            ax.flat[d].set_xlabel(\"Iteration\")\n",
    "            ax.flat[d].set_ylabel(\"Number of cases\")\n",
    "    #fig.set_title(f\"Num {disease_status} people by age group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disease_status_by_age(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot MSOA geodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load MSOA shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsim.load_msoa_locations import load_osm_shapefile, load_msoa_shapes\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = (\"../devon_data\")\n",
    "\n",
    "osm_buildings = load_osm_shapefile(data_dir)\n",
    "\n",
    "devon_msoa_shapes = load_msoa_shapes(data_dir, visualize=False)\n",
    "\n",
    "devon_msoa_shapes.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_msoa_choropleth(msoa_shapes, summary, disease_status, timestep):\n",
    "    # get dataframes for all statuses\n",
    "    msoa_data = summary.get_area_dataframes()\n",
    "    \n",
    "    msoa_data_for_status = msoa_data[disease_status]\n",
    "\n",
    "    # add \"Code\" column so dataframes can be merged\n",
    "    msoa_data_for_status[\"Code\"] = msoa_data_for_status.index\n",
    "    msoa_shapes = pd.merge(msoa_shapes, msoa_data_for_status, on=\"Code\")\n",
    "\n",
    "    msoa_shapes.plot(column=f\"Day{timestep}\", legend=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot disease status by MSOA for a given timestep and status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_status = \"exposed\"\n",
    "\n",
    "plot_msoa_choropleth(devon_msoa_shapes, summaries[0], disease_status, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Data\n",
    "\n",
    "Read the real observations (number of hospital admissions in Devon) that will be used to calibrate the model. See the [README](./observation_data/README.md) for information about how these observations were obtained. They aren't the raw cases, it's actually a model that was fitted to the lagged cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>103</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Day  Cases\n",
       "0      1     10\n",
       "1      2     11\n",
       "2      3     13\n",
       "3      4     16\n",
       "4      5     18\n",
       "..   ...    ...\n",
       "98    99     10\n",
       "99   100      9\n",
       "100  101      9\n",
       "101  102      8\n",
       "102  103      8\n",
       "\n",
       "[103 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations = pd.read_csv(\"observation_data/gam_cases.csv\", header=0, names=[\"Day\", \"Cases\"], )\n",
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Error: Fitness Function\n",
    "\n",
    "To calibrate the model we need a fitness function that tells us, for a given result, how similar it is to the observations. There are lots of ways to do this. For now, just take the **Euclidean distance (L2 norm)** between the observed number of cases and the simulated number of cases.\n",
    "\n",
    "Note that the model is seeded using the first few days of cases, so at the beginning of a run the simulated data will be identical to the observations. This doesn't matter though because the relative difference between different parameter combinations will be the same regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fitness function is defined in experiments_functions.py (so that it can be tested)\n",
    "fit = Functions.fit_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Parameters\n",
    "\n",
    "Which parameters will we try to calibrate on?\n",
    "\n",
    "To begin with lets just try the `current_risk_beta` (a general multiplier for risk at locations).\n",
    "\n",
    "Create a function that takes a value of that parameter as an argument, runs a model, and returns the result (the number of cases per day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 100  # Number of iterations to run for\n",
    "NUM_SEED_DAYS = 10  # Number of days to seed the population\n",
    "USE_GPU = False\n",
    "STORE_DETAILED_COUNTS = False\n",
    "REPETITIONS = 5 \n",
    "\n",
    "assert ITERATIONS < len(observations), \\\n",
    "    f\"Have more iterations ({ITERATIONS}) than observations ({len(observations)}).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_current_risk_beta(x, return_full_details=False):\n",
    "    \"\"\"Run a model REPETITIONS times using the provided value for the current_risk_beta. \n",
    "    \n",
    "    :param return_details: If True then rather than just returning the fitness, \n",
    "        return a tuple of (fitness, summaries_list, final_results_list).\n",
    "    :return: The mean fitness across all model runs\n",
    "    \n",
    "    \"\"\"\n",
    "    # Sometimes x might be passed as a number in a list\n",
    "    if isinstance(x, np.ndarray) or isinstance(x, list):\n",
    "        if len(x) > 1:\n",
    "            raise Exception(\n",
    "                f\"The curent risk beta value (x) should either be a 1-element array or a single number, not {x}\")\n",
    "        x = x[0]\n",
    "\n",
    "    params = Functions.create_parameters(parameters_file=os.path.join(\"../..\", \"model_parameters\", \"default.yml\"), current_risk_beta=x)\n",
    "    \n",
    "    results = Functions.run_opencl_model_multi(\n",
    "        repetitions=REPETITIONS, iterations=ITERATIONS, params=params,\n",
    "        opencl_dir=os.path.join(\"../..\", \"microsim\", \"opencl\"),\n",
    "        snapshot_filepath=os.path.join(\"../..\", \"microsim\", \"opencl\", \"snapshots\", \"cache.npz\"),\n",
    "        multiprocess=False\n",
    "    )\n",
    "    \n",
    "    summaries = [x[0] for x in results]\n",
    "    final_results = [x[1] for x in results]\n",
    "\n",
    "    # Get mean cases per day from the summary object\n",
    "    sim = Functions.get_mean_total_counts(summaries, DiseaseStatus.Exposed.value)\n",
    "    # Compare these to the observations\n",
    "    obs = observations.loc[:ITERATIONS-1,\"Cases\"].values\n",
    "    assert len(sim) == len(obs)\n",
    "    fitness = fit(sim,obs)\n",
    "    if return_full_details:\n",
    "        return ( fitness, sim, obs )\n",
    "    else:\n",
    "        return fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.73s)\n",
      "fitness: 980.9741484871047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(fitness, sim, obs) = run_model_current_risk_beta(0.001, return_full_details=True)\n",
    "#fitness = run_model_current_risk_beta(0.001)\n",
    "\n",
    "print(f\"fitness: {fitness}\")\n",
    "#list(zip(obs,sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Method: XXXX\n",
    "\n",
    "Lots of different methods are avialbale. Simulated annealing? Latin-Hypercube sampling? GA? ABC?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a simple minimisation algorithm [Nelder-Mead Simplex algorithm](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#nelder-mead-simplex-algorithm-method-nelder-mead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running models: 100%|██████████| 5/5 [00:15<00:00,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 15.79s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:18<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 18.47s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:16<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 16.43s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:15<00:00,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 15.27s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:14<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 14.99s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:15<00:00,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 15.42s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:14<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 14.22s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.67s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:16<00:00,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 16.33s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.95s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.37s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:16<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 16.46s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:16<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 16.36s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.45s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.47s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.4s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.54s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.41s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.88s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.86s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.89s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.77s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:14<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 14.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.87s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.76s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.71s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.93s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.89s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.81s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.65s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.34s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.54s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.45s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.39s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:14<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 14.02s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.74s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.35s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.39s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.29s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.47s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.45s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.53s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.51s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.42s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.57s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.4s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.58s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.45s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.66s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models: 100%|██████████| 5/5 [00:13<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. finished, took 13.9s)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 957.935113\n",
      "         Iterations: 23\n",
      "         Function evaluations: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "x0 = np.array([0.005])  # initial guess for each variable\n",
    "\n",
    "optimisation_result = minimize(run_model_current_risk_beta, x0, method='nelder-mead',\n",
    "               options={'xatol': 1e-8, 'disp': True})\n",
    "\n",
    "pickle.dump( optimisation_result, open( \"./optimisation_result.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00096506])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimisation_result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests\n",
    "\n",
    "I've not put tests into a notebook before. Not sure how well this will work. Uses a small ipython extension: https://github.com/akaihola/ipython_pytest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
